---
layout: post
title: Variational Inference Algorithm
---
<div class="entry-content">
		
<p style="font-size:18px;">Sampling data or approximating probabilistic densities is one of the core problems in modern statistics, especially in Bayesian statistic. Beside of MCMC, Variational Inference is one of the two typical Bayesian approaches solving this problem. It leverages the Variational Inference algorithm to develop many recent powerful methods and models: Stochastic Variational Inference, Variational Autoencoder&#8230; This article briefly introduces Variational Inference algorithm.</p>



<p style="font-size:18px;">The main ideas of Variational Inference (VI) is approximating the posterior with simpler variational distribution by solving optimization problem. The section 1 reviews Bayesian inference and latent variables. Section 2 presents objective of the algorithm and why it solves optimization problem. Section 3 shows some important mathematical contributions while attempting the target. Section 4 introduces Stochastic Variational Inference and finally  the summary is presented.</p>



<h2>1. Bayesian Inference and Latent variables</h2>



<ul><li><strong><em>Bayesian Inference</em></strong></li></ul>



<p style="font-size:18px;">Bayesian inference supposes the probability of observations <img src="https://s0.wp.com/latex.php?latex=x&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="x" title="x" class="latex" /> or tractable distributions depend on a quantity, called prior. Prior reflects personal perspective about observations, comes from knowledge of experts or tuning through experiments. Let denote it as <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;theta" title="&#92;theta" class="latex" />. </p>



<p style="font-size:18px;" class="has-text-align-center">Prior: <img src="https://s0.wp.com/latex.php?latex=p%28%5Ctheta%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="p(&#92;theta)" title="p(&#92;theta)" class="latex" /></p>



<p style="font-size:18px;">Given <img src="https://s0.wp.com/latex.php?latex=theta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="theta" title="theta" class="latex" />, we can figure out what probability of outcome by a evidence function: </p>



<p style="font-size:18px;" class="has-text-align-center">Likelihood: <img src="https://s0.wp.com/latex.php?latex=p%28x%7C%5Ctheta%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="p(x|&#92;theta)" title="p(x|&#92;theta)" class="latex" /></p>



<p style="font-size:18px;">We don&#8217;t always know exact values of prior. Therefore, we have the hypothesis reflecting how much <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;theta" title="&#92;theta" class="latex" /> is matched with <em>the true prior</em>. With Bayes theorem: <img src="https://s0.wp.com/latex.php?latex=p%28A%7CB%29%3D%5Cdfrac%7Bp%28B%7CA%29p%28A%29%7D%7Bp%28B%29%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="p(A|B)=&#92;dfrac{p(B|A)p(A)}{p(B)}" title="p(A|B)=&#92;dfrac{p(B|A)p(A)}{p(B)}" class="latex" />, the posterior is:</p>



<p style="font-size:18px;" class="has-text-align-center">Posterior: <img src="https://s0.wp.com/latex.php?latex=p%28%5Ctheta%7Cx%29+%3D+%5Cdfrac%7Bp%28x%7C%5Ctheta%29p%28%5Ctheta%29%7D%7Bp%28x%29%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="p(&#92;theta|x) = &#92;dfrac{p(x|&#92;theta)p(&#92;theta)}{p(x)}" title="p(&#92;theta|x) = &#92;dfrac{p(x|&#92;theta)p(&#92;theta)}{p(x)}" class="latex" /></p>



<p style="font-size:18px;">To test which parameters is best in describing the data, methods find which one has the maximum value of the likelihood or the posterior. If you&#8217;re not familiar with Bayesian inference, please read this <a href="https://langocthuyan.wordpress.com/2020/03/23/getting-started-with-bayesian-inference/"><strong><em>post</em></strong></a>.</p>



<ul><li><strong><em>Latent variables</em></strong></li></ul>



<blockquote class="wp-block-quote" style="font-size:14px;"><p>In statistics, latent variables are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured). Mathematical models that aim to explain observed variables in terms of latent variables are called latent variable models.</p><cite>See <a href="https://en.wikipedia.org/wiki/Latent_variable">Wikipedia</a></cite></blockquote>



<p style="font-size:18px;">Latent variables are not directly observed but we have information about their structure and relationship to the observations. Thus, it can be inferred. In Bayesian inference, they are often factors that allow us to factorize the observations to tractable distributions. </p>



<p style="font-size:18px;">For example, in topic models, the only observation variable is collections of documents with their words. The list of topics, the distribution between topics and documents or words, both of them are latent variables.</p>



<h2>2. Model&#8217;s objective and optimization problem</h2>



<p style="font-size:18px;">Variational Inference strives to sample observations or difficult-to-compute distribution. It uses hidden variables to encode hidden structure in observed data and estimates the posterior. The posterior is not always tractable, therefore, more generic way is approximating it by simpler distribution governed by variational parameters.</p>



<p style="font-size:18px;" class="has-text-align-center">Goal distribution: the posterior <img src="https://s0.wp.com/latex.php?latex=p%28z%7Cx%29+%3D+%5Cdfrac%7Bp%28x%2C+z%29%7D%7B%5Cint+p%28x%2Cz%29+dz%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="p(z|x) = &#92;dfrac{p(x, z)}{&#92;int p(x,z) dz}" title="p(z|x) = &#92;dfrac{p(x, z)}{&#92;int p(x,z) dz}" class="latex" /></p>



<p style="font-size:18px;" class="has-text-align-center">Approximating distribution: the variational distribution <img src="https://s0.wp.com/latex.php?latex=q%28z%3B%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="q(z;&#92;lambda)" title="q(z;&#92;lambda)" class="latex" />, <br>where <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda" title="&#92;lambda" class="latex" /> is a set of variational parameters.</p>



<p style="font-size:18px;" class="has-text-align-center">Loss function: the negative log likelihood <img src="https://s0.wp.com/latex.php?latex=L%28%5Clambda%29+%3D+-+log+p%28x%7C%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="L(&#92;lambda) = - log p(x|&#92;lambda)" title="L(&#92;lambda) = - log p(x|&#92;lambda)" class="latex" /></p>



<p style="font-size:18px;">VI measures the distance between distributions by KL divergence:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=D_%7BKL%7D%5Cbigg%28q%28z%29+%7C%7C+p%28z%29%5Cbigg%29+%3D+-%5Cint+q%28z%29+log+%5Cdfrac%7Bp%28z%29%7D%7Bq%28z%29%7D+dz&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="D_{KL}&#92;bigg(q(z) || p(z)&#92;bigg) = -&#92;int q(z) log &#92;dfrac{p(z)}{q(z)} dz" title="D_{KL}&#92;bigg(q(z) || p(z)&#92;bigg) = -&#92;int q(z) log &#92;dfrac{p(z)}{q(z)} dz" class="latex" /></p>



<p style="font-size:18px;">Lower distance means better approximation, thus VI minimizes the distance and the problem becomes finding parameters which have optimal values on the targeting metric.</p>



<p style="font-size:18px;">Instead of directly minimizing <img src="https://s0.wp.com/latex.php?latex=D_%7BKL%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="D_{KL}" title="D_{KL}" class="latex" />, authors maximizes ELBO <img src="https://s0.wp.com/latex.php?latex=L%28%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="L(&#92;lambda)" title="L(&#92;lambda)" class="latex" />. For short, this term has close relation to <img src="https://s0.wp.com/latex.php?latex=D_%7BKL%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="D_{KL}" title="D_{KL}" class="latex" /> and log likelihood <img src="https://s0.wp.com/latex.php?latex=p%28x%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="p(x)" title="p(x)" class="latex" />:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=log+p%28x%29+%3D+L%28%5Clambda%29+%2B+D_%7BKL%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="log p(x) = L(&#92;lambda) + D_{KL}" title="log p(x) = L(&#92;lambda) + D_{KL}" class="latex" /></p>



<h2>3. Mathematical notes</h2>



<p style="font-size:18px;">This section explains in detail mathematical points and how VI solves optimization problems.</p>



<h3>3.1 <strong><em>Evidence Lower Bound</em></strong> <strong><em>(ELBO)</em></strong></h3>



<p style="font-size:18px;">This term is called ELBO since it is the lower bound of evidence function of data:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=log+p%28x%29%3D+log+%5Cint+p%28x%2Cz%29+dz+%3D+log+%5Cint+%5Cdfrac%7Bp%28x%2Cz%29q%28z%3B%5Clambda%29%7D%7Bq%28z%3B+%5Clambda%29%7D+dz+%3D+log+E_%7Bq%28z%3B%5Clambda%29%7D%5Cbigg%5B%5Cdfrac%7Bp%28x%2Cz%29%7D%7Bq%28z%3B+%5Clambda%29%7D+%5Cbigg%5D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="log p(x)= log &#92;int p(x,z) dz = log &#92;int &#92;dfrac{p(x,z)q(z;&#92;lambda)}{q(z; &#92;lambda)} dz = log E_{q(z;&#92;lambda)}&#92;bigg[&#92;dfrac{p(x,z)}{q(z; &#92;lambda)} &#92;bigg]" title="log p(x)= log &#92;int p(x,z) dz = log &#92;int &#92;dfrac{p(x,z)q(z;&#92;lambda)}{q(z; &#92;lambda)} dz = log E_{q(z;&#92;lambda)}&#92;bigg[&#92;dfrac{p(x,z)}{q(z; &#92;lambda)} &#92;bigg]" class="latex" /></p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cgeq+E_%7Bq%28z%3B%5Clambda%29%7D%5Cbigg%5Blog+%5Cdfrac%7Bp%28x%2Cz%29%7D%7Bq%28z%3B+%5Clambda%29%7D+%5Cbigg%5D+%3D+ELBO%28%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;geq E_{q(z;&#92;lambda)}&#92;bigg[log &#92;dfrac{p(x,z)}{q(z; &#92;lambda)} &#92;bigg] = ELBO(&#92;lambda)" title="&#92;geq E_{q(z;&#92;lambda)}&#92;bigg[log &#92;dfrac{p(x,z)}{q(z; &#92;lambda)} &#92;bigg] = ELBO(&#92;lambda)" class="latex" /> (applying Jensen&#8217;inequality)</p>



<p style="font-size:18px;">Proof of <img src="https://s0.wp.com/latex.php?latex=log+p%28x%29+%3D+ELBO+%2B+D_%7BKL%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="log p(x) = ELBO + D_{KL}" title="log p(x) = ELBO + D_{KL}" class="latex" /> and applying Jensen&#8217;inequality in ELBO, see Section 3 of this <a href="https://langocthuyan.wordpress.com/2020/03/15/jensens-inequality-and-applications-em-algorithm-and-elbo-in-svi/"><em><strong>post</strong></em></a>.</p>



<h3>3.2 <strong><em>Mean-field variational inference</em></strong></h3>



<p style="font-size:18px;">To express <img src="https://s0.wp.com/latex.php?latex=q%28z%3B%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="q(z;&#92;lambda)" title="q(z;&#92;lambda)" class="latex" /> as a function of <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda" title="&#92;lambda" class="latex" />, there are many ways, as well as the tradeoff between expressive to approximate and simple to track. The mean-field theory assumes that all variables are independent to each other, therefore correlation information between them is lost. In VI, the distribution is factorized into factors and each other is governed by its own parameters:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=q%28z%3B+%5Clambda%29+%3D+%5CPi_%7Bi%3D1%7D%5E%7BN%7Dq%28z_i%3B%5Clambda_i%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="q(z; &#92;lambda) = &#92;Pi_{i=1}^{N}q(z_i;&#92;lambda_i)" title="q(z; &#92;lambda) = &#92;Pi_{i=1}^{N}q(z_i;&#92;lambda_i)" class="latex" /></p>



<p style="font-size:18px;">Due to this assumption, VI updates iteratively by pairs <img src="https://s0.wp.com/latex.php?latex=%28z_i%2C+%5Clambda_i%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="(z_i, &#92;lambda_i)" title="(z_i, &#92;lambda_i)" class="latex" /> to optimize <img src="https://s0.wp.com/latex.php?latex=L%28%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="L(&#92;lambda)" title="L(&#92;lambda)" class="latex" /> by gradient descent or coordinate descent.</p>


<pre class="brush: python; title: ; notranslate" title="">
while (ELBO has not convergenced)
	compute ELBO (q)    
	for j in {1, .. m}:
		q_j(z_j) = q_j*(z_j) # optimal value of q_j(z_j)
</pre>


<p style="font-size:18px;">Coordinate descent finds the steepest values of each variable while keeping others fixed, then updates the steepest to the variable. In contrast, gradient descent optimizes the whole dimensions at the same time, but with a small ratio of magnitude rather than full length of distant from current points to optimal values. Each time updating all factors, despite independently or simultaneously, the lower bound (ELBO) is increased, the training process found the better local optimum.</p>



<p style="font-size:18px;">Both gradient descent or coordinate descent work by finding steepest values of local convex loss function.  How to find them?</p>



<h3>3.3 <strong><em>Natural gradient</em></strong></h3>



<p style="font-size:18px;">The important thing to optimize the loss function <img src="https://s0.wp.com/latex.php?latex=L%28%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="L(&#92;lambda)" title="L(&#92;lambda)" class="latex" /> by VI is finding optimal value on each updating step. If the loss function is in Euclidean space, it is easy to compute the distance by simply taking the first-order gradient. However, the parameters are still in Euclidean space, but the distance metric is not. Comparing between 2 distributions with just subtraction their mean is insufficient. A lot of information, such as shape, variation, skew&#8230; of distributions are not considered.  Therefore, we need another way to compare distance metric in distribution space, then indicate how to update parameters on Euclidean space. This is done by natural gradient. </p>



<ul><li><strong><em>Natural gradient</em></strong> and <strong><em>Fisher Information</em></strong></li></ul>



<p style="font-size:18px;">Natural gradient is proven to be extracted by the equation:</p>



<p style="font-size:18px;" class="has-text-align-center"> <img src="https://s0.wp.com/latex.php?latex=%5Chat%5Cnabla+L%28%5Clambda%29+%3D+F%5E%7B-1%7D%5Cnabla+L%28%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;hat&#92;nabla L(&#92;lambda) = F^{-1}&#92;nabla L(&#92;lambda)" title="&#92;hat&#92;nabla L(&#92;lambda) = F^{-1}&#92;nabla L(&#92;lambda)" class="latex" /><br>where <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="F" title="F" class="latex" /> is the Fisher Information of the loss function <img src="https://s0.wp.com/latex.php?latex=L%28%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="L(&#92;lambda)" title="L(&#92;lambda)" class="latex" />.- </p>



<p style="font-size:18px;">The Fisher Information matrix is computed by getting second-order gradient, Hessian matrix, of the <img src="https://s0.wp.com/latex.php?latex=D_%7BKL%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="D_{KL}" title="D_{KL}" class="latex" />. </p>



<p style="font-size:18px;">Let <img src="https://s0.wp.com/latex.php?latex=%7BP_%5Ctheta%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="{P_&#92;theta}" title="{P_&#92;theta}" class="latex" /> denote a parametric family of a distribution on space <img src="https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="X" title="X" class="latex" />. Density function: <img src="https://s0.wp.com/latex.php?latex=p_%5Ctheta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="p_&#92;theta" title="p_&#92;theta" class="latex" />. Equation of Fisher information:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=F_%5Ctheta+%3A%3D+E_%5Ctheta+%5Cbigg%5B+%5Cnabla_%5Ctheta+log+%28p_%5Ctheta%28x%29%29+%5Cnabla_%5Ctheta+log%28p_%5Ctheta%28x%29%29%5ET+%5Cbigg%5D+%3D+-E_%5Ctheta+%5Cbigg%5B+%5Cnabla_%5Ctheta%5E2+log%28p_%5Ctheta%28x%29%29+%5Cbigg%5D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="F_&#92;theta := E_&#92;theta &#92;bigg[ &#92;nabla_&#92;theta log (p_&#92;theta(x)) &#92;nabla_&#92;theta log(p_&#92;theta(x))^T &#92;bigg] = -E_&#92;theta &#92;bigg[ &#92;nabla_&#92;theta^2 log(p_&#92;theta(x)) &#92;bigg]" title="F_&#92;theta := E_&#92;theta &#92;bigg[ &#92;nabla_&#92;theta log (p_&#92;theta(x)) &#92;nabla_&#92;theta log(p_&#92;theta(x))^T &#92;bigg] = -E_&#92;theta &#92;bigg[ &#92;nabla_&#92;theta^2 log(p_&#92;theta(x)) &#92;bigg]" class="latex" /></p>



<ul><li><strong><em>Proof <img src="https://s0.wp.com/latex.php?latex=F%3D+H_%7BKL%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="F= H_{KL}" title="F= H_{KL}" class="latex" /></em></strong></li></ul>



<p style="font-size:18px;" class="has-text-align-center">Suppose that <img src="https://s0.wp.com/latex.php?latex=p%28x%7C%5Clambda%5E%2A%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="p(x|&#92;lambda^*)" title="p(x|&#92;lambda^*)" class="latex" /> is the true posterior, and <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E%2A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda^*" title="&#92;lambda^*" class="latex" /> is the optimal value of <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda" title="&#92;lambda" class="latex" />. </p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=D_%7BKL%7D%5Cbigg%28+p%28x%7C%5Clambda%5E%2A%29+%7C%7C+p%28x%7C%5Clambda%29+%5Cbigg%29+%3D++E_%7Bp%28x%7C%5Clambda%29%7D%5Cbig%5B+log+%28p%28x%7C%5Clambda%5E%2A+%29%5Cbig%29+%5Cbig%5D+-+E_%7Bp%28x%7C%5Clambda%29%7D%5Cbig%5B+log+%28p%28x%7C%5Clambda+%29%5Cbig%29+%5Cbig%5D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="D_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) =  E_{p(x|&#92;lambda)}&#92;big[ log (p(x|&#92;lambda^* )&#92;big) &#92;big] - E_{p(x|&#92;lambda)}&#92;big[ log (p(x|&#92;lambda )&#92;big) &#92;big]" title="D_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) =  E_{p(x|&#92;lambda)}&#92;big[ log (p(x|&#92;lambda^* )&#92;big) &#92;big] - E_{p(x|&#92;lambda)}&#92;big[ log (p(x|&#92;lambda )&#92;big) &#92;big]" class="latex" /></p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cnabla_%5Clambda%5E1+D_%7BKL%7D%5Cbigg%28+p%28x%7C%5Clambda%5E%2A%29+%7C%7C+p%28x%7C%5Clambda%29+%5Cbigg%29+%3D+%5Cnabla_%5Clambda%5E1+E_%7Bp%28x%7C%5Clambda%29%7D%5Cbig%5B+log+%28p%28x%7C%5Clambda%5E%2A+%29%5Cbig%29+%5Cbig%5D+-+%5Cnabla_%5Clambda%5E1+E_%7Bp%28x%7C%5Clambda%29%7D%5Cbig%5B+log+%28p%28x%7C%5Clambda%29%5Cbig%29+%5Cbig%5D+&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;nabla_&#92;lambda^1 D_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) = &#92;nabla_&#92;lambda^1 E_{p(x|&#92;lambda)}&#92;big[ log (p(x|&#92;lambda^* )&#92;big) &#92;big] - &#92;nabla_&#92;lambda^1 E_{p(x|&#92;lambda)}&#92;big[ log (p(x|&#92;lambda)&#92;big) &#92;big] " title="&#92;nabla_&#92;lambda^1 D_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) = &#92;nabla_&#92;lambda^1 E_{p(x|&#92;lambda)}&#92;big[ log (p(x|&#92;lambda^* )&#92;big) &#92;big] - &#92;nabla_&#92;lambda^1 E_{p(x|&#92;lambda)}&#92;big[ log (p(x|&#92;lambda)&#92;big) &#92;big] " class="latex" /></p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%3D+-+E_%7Bp%28x%7C%5Clambda%29%7D%5Cbig%5B%5Cnabla_%5Clambda%5E1+log+%28p%28x%7C%5Clambda+%29%5Cbig%29+%5Cbig%5D+&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="= - E_{p(x|&#92;lambda)}&#92;big[&#92;nabla_&#92;lambda^1 log (p(x|&#92;lambda )&#92;big) &#92;big] " title="= - E_{p(x|&#92;lambda)}&#92;big[&#92;nabla_&#92;lambda^1 log (p(x|&#92;lambda )&#92;big) &#92;big] " class="latex" /></p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Cnabla_%5Clambda%5E2+D_%7BKL%7D%5Cbigg%28+p%28x%7C%5Clambda%5E%2A%29+%7C%7C+p%28x%7C%5Clambda%29+%5Cbigg%29+%3D+-+E_%7Bp%28x%7C%5Clambda%29%7D%5Cbig%5B%5Cnabla_%5Clambda%5E2+log+%28p%28x%7C%5Clambda%29%5Cbig%29+%5Cbig%5D+%3D+-+%5Cint+p%28x%7C%5Clambda%29+%5Cnabla_%5Clambda%5E2+log+%28p%28x%7C%5Clambda%29%5Cbig%29+dx&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;nabla_&#92;lambda^2 D_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) = - E_{p(x|&#92;lambda)}&#92;big[&#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda)&#92;big) &#92;big] = - &#92;int p(x|&#92;lambda) &#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda)&#92;big) dx" title="&#92;nabla_&#92;lambda^2 D_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) = - E_{p(x|&#92;lambda)}&#92;big[&#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda)&#92;big) &#92;big] = - &#92;int p(x|&#92;lambda) &#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda)&#92;big) dx" class="latex" /></p>



<p style="font-size:18px;">Evaluate <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda" title="&#92;lambda" class="latex" /> at <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E%2A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda^*" title="&#92;lambda^*" class="latex" />:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=H_%7BKL%7D%5Cbigg%28+p%28x%7C%5Clambda%5E%2A%29+%7C%7C+p%28x%7C%5Clambda%29+%5Cbigg%29+%3D+-+%5Cint+p%28x%7C%5Clambda%29+%5Cnabla_%5Clambda%5E2+log+%28p%28x%7C%5Clambda%29%5Cbig%29+%5Cbig%7C_%7B%5Clambda%3D%5Clambda%5E%2A%7D+dx+%3D+-+%5Cint+p%28x%7C%5Clambda%5E%2A%29+%5Cnabla_%5Clambda%5E2+log+%28p%28x%7C%5Clambda%5E%2A%29%5Cbig%29+dx&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="H_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) = - &#92;int p(x|&#92;lambda) &#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda)&#92;big) &#92;big|_{&#92;lambda=&#92;lambda^*} dx = - &#92;int p(x|&#92;lambda^*) &#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda^*)&#92;big) dx" title="H_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) = - &#92;int p(x|&#92;lambda) &#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda)&#92;big) &#92;big|_{&#92;lambda=&#92;lambda^*} dx = - &#92;int p(x|&#92;lambda^*) &#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda^*)&#92;big) dx" class="latex" /></p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%3D+-E_%7Bp%28x%7C%5Clambda%5E%2A%29%7D%5Cbigg%5B+%5Cnabla_%5Clambda%5E2+log+%28p%28x%7C%5Clambda%5E%2A%29%5Cbig%29+%5Cbigg%5D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="= -E_{p(x|&#92;lambda^*)}&#92;bigg[ &#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda^*)&#92;big) &#92;bigg]" title="= -E_{p(x|&#92;lambda^*)}&#92;bigg[ &#92;nabla_&#92;lambda^2 log (p(x|&#92;lambda^*)&#92;big) &#92;bigg]" class="latex" /></p>



<p style="font-size:18px;" class="has-text-align-center">Thus, <img src="https://s0.wp.com/latex.php?latex=H_%7BKL%7D%5Cbigg%28+p%28x%7C%5Clambda%5E%2A%29+%7C%7C+p%28x%7C%5Clambda%29+%5Cbigg%29+%3D+F_%7Bp%28x%7C%5Clambda%5E%2A%29%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="H_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) = F_{p(x|&#92;lambda^*)}" title="H_{KL}&#92;bigg( p(x|&#92;lambda^*) || p(x|&#92;lambda) &#92;bigg) = F_{p(x|&#92;lambda^*)}" class="latex" /></p>



<ul><li><strong><em>Finding steepest points</em></strong></li></ul>



<p style="font-size:18px;">Let <img src="https://s0.wp.com/latex.php?latex=KL%28%5Clambda%27%29+%3D+D_%7BKL%7D%5Cbigg%28+p%28x%7C%5Clambda%27%29+%7C%7C+p%28x%7C%5Clambda%29%5Cbigg%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="KL(&#92;lambda&#039;) = D_{KL}&#92;bigg( p(x|&#92;lambda&#039;) || p(x|&#92;lambda)&#92;bigg)" title="KL(&#92;lambda&#039;) = D_{KL}&#92;bigg( p(x|&#92;lambda&#039;) || p(x|&#92;lambda)&#92;bigg)" class="latex" />. <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E%2A+%3D+%5Clambda+%2B+%5Cnabla+KL%28%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda^* = &#92;lambda + &#92;nabla KL(&#92;lambda)" title="&#92;lambda^* = &#92;lambda + &#92;nabla KL(&#92;lambda)" class="latex" />. Finding <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E%2A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda^*" title="&#92;lambda^*" class="latex" /> means minimizing a function in distribution space:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=min+%5Cbig%5C%7BKL%28%5Clambda+%2B+%5Cnabla+KL%28%5Clambda%29%29%5Cbig%5C%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="min &#92;big&#92;{KL(&#92;lambda + &#92;nabla KL(&#92;lambda))&#92;big&#92;}" title="min &#92;big&#92;{KL(&#92;lambda + &#92;nabla KL(&#92;lambda))&#92;big&#92;}" class="latex" /></p>



<p style="font-size:18px;">With second-order Taylor Series:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=f%28x+%2B+dx%29+%5Capprox+f%28x%29+%2B+%5Cnabla+f%28x%29%5ET+%5CDelta+x++%2B+%5Cdfrac%7B1%7D%7B2%7D%5CDelta+x%5ET+H%28x%29%5CDelta+x&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="f(x + dx) &#92;approx f(x) + &#92;nabla f(x)^T &#92;Delta x  + &#92;dfrac{1}{2}&#92;Delta x^T H(x)&#92;Delta x" title="f(x + dx) &#92;approx f(x) + &#92;nabla f(x)^T &#92;Delta x  + &#92;dfrac{1}{2}&#92;Delta x^T H(x)&#92;Delta x" class="latex" /></p>



<p style="font-size:18px;">Let <img src="https://s0.wp.com/latex.php?latex=d+%3D+%5Cnabla+KL%28%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="d = &#92;nabla KL(&#92;lambda)" title="d = &#92;nabla KL(&#92;lambda)" class="latex" />. We have:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=KL+%28%5Clambda%5E%2A%29+%3D+KL%28%5Clambda+%2B+d%29+%5Capprox+KL%28%5Clambda%29+%2B+%5Cnabla+KL%28%5Clambda%29%5ETd+%2B+%5Cdfrac%7B1%7D%7B2%7Dd%5ET+F+d+&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="KL (&#92;lambda^*) = KL(&#92;lambda + d) &#92;approx KL(&#92;lambda) + &#92;nabla KL(&#92;lambda)^Td + &#92;dfrac{1}{2}d^T F d " title="KL (&#92;lambda^*) = KL(&#92;lambda + d) &#92;approx KL(&#92;lambda) + &#92;nabla KL(&#92;lambda)^Td + &#92;dfrac{1}{2}d^T F d " class="latex" /></p>



<p style="font-size:18px;"><img src="https://s0.wp.com/latex.php?latex=KL%28%5Clambda%29+%3D+0&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="KL(&#92;lambda) = 0" title="KL(&#92;lambda) = 0" class="latex" /> because it is the distance from <img src="https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda" title="&#92;lambda" class="latex" /> to itself, thus:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=KL+%28%5Clambda%5E%2A%29+%3D+KL%28%5Clambda%2Bd%29+%5Capprox+%5Cdfrac%7B1%7D%7B2%7Dd%5ET+F+d+&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="KL (&#92;lambda^*) = KL(&#92;lambda+d) &#92;approx &#92;dfrac{1}{2}d^T F d " title="KL (&#92;lambda^*) = KL(&#92;lambda+d) &#92;approx &#92;dfrac{1}{2}d^T F d " class="latex" /></p>



<p style="font-size:18px;">To find <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5E%2A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;lambda^*" title="&#92;lambda^*" class="latex" />, corresponding to minimizing the loss function <img src="https://s0.wp.com/latex.php?latex=KL&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="KL" title="KL" class="latex" />, we minimize the <img src="https://s0.wp.com/latex.php?latex=L+%3D+-log+p%28x%7C%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="L = -log p(x|&#92;lambda)" title="L = -log p(x|&#92;lambda)" class="latex" />: </p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=min+%5Cbig%5C%7BL%28%5Clambda+%2B+%5Cnabla+L%28%5Clambda%29%29%5Cbig%5C%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="min &#92;big&#92;{L(&#92;lambda + &#92;nabla L(&#92;lambda))&#92;big&#92;}" title="min &#92;big&#92;{L(&#92;lambda + &#92;nabla L(&#92;lambda))&#92;big&#92;}" class="latex" /></p>



<p style="font-size:18px;">Let <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bd%7D+%3D%5Cnabla+L%28%5Clambda%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;hat{d} =&#92;nabla L(&#92;lambda)" title="&#92;hat{d} =&#92;nabla L(&#92;lambda)" class="latex" />. It was proved that <img src="https://s0.wp.com/latex.php?latex=L%28%5Clambda+%2B%5Chat%7Bd%7D%29+%3D+min+%5Ciff+%5Chat%7Bd%7D+%3D+KL+%28%5Clambda%5E%2A%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="L(&#92;lambda +&#92;hat{d}) = min &#92;iff &#92;hat{d} = KL (&#92;lambda^*)" title="L(&#92;lambda +&#92;hat{d}) = min &#92;iff &#92;hat{d} = KL (&#92;lambda^*)" class="latex" />:</p>



<p style="font-size:18px;" class="has-text-align-center"><img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bd%7D+%3D+KL+%28%5Clambda%5E%2A%29+%5CRightarrow+d+%3D+F%5E%7B-1%7D%5Chat%7Bd%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;hat{d} = KL (&#92;lambda^*) &#92;Rightarrow d = F^{-1}&#92;hat{d}" title="&#92;hat{d} = KL (&#92;lambda^*) &#92;Rightarrow d = F^{-1}&#92;hat{d}" class="latex" />  or <img src="https://s0.wp.com/latex.php?latex=%5Chat%5Cnabla_%5Ctheta%7BL%7D+%3D+F%5E%7B-1%7D%5Cnabla_%5Ctheta%7BL%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;hat&#92;nabla_&#92;theta{L} = F^{-1}&#92;nabla_&#92;theta{L}" title="&#92;hat&#92;nabla_&#92;theta{L} = F^{-1}&#92;nabla_&#92;theta{L}" class="latex" />.</p>



<p style="font-size:18px;">The term <img src="https://s0.wp.com/latex.php?latex=%5Chat%5Cnabla_%5Ctheta%7BL%7D+%3D+F%5E%7B-1%7D%5Cnabla_%5Ctheta%7BL%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="&#92;hat&#92;nabla_&#92;theta{L} = F^{-1}&#92;nabla_&#92;theta{L}" title="&#92;hat&#92;nabla_&#92;theta{L} = F^{-1}&#92;nabla_&#92;theta{L}" class="latex" /> is called natural gradient. In fact, <img src="https://s0.wp.com/latex.php?latex=F&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002" alt="F" title="F" class="latex" /> is a Riemannian metric performing transformation from distribution space to Euclidean space.</p>



<p style="font-size:18px;">More detail in explaining natural gradient descent, check out <a href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/"><em><strong>here</strong></em></a>.</p>



<h2>4. Stochastic Variational Inference (SVI)</h2>



<p style="font-size:18px;">SVI was inspired by Stochastic Gradient Descent, which separates the training set into many sub-set and feeds data to neural network with smaller size. This is helpful to save resource usage so that SVI is highly scalable while traditional method could fail to be trained because of loading full size of data. The challange of SVI is how to approximate the natural gradients of full data by natural gradients of small sub-set. They introduce global and local parameters for all hidden variables, including all latent variables and all their parameters. The global holds natural gradients of full dataset, while the local saves natural gradients of current sub-set. </p>



<h2>Summary</h2>



<p style="font-size:18px;">Variational Inference methods use hidden variables to encode hidden structure in observed data. The relationship between the hidden variables and observations is factorized by joint probability distribution. Therefore, to sample observations, we estimate the posterior distribution (the conditional distribution of the hidden structure given the observations). This posterior could be not tractable to compute and we must appeal to approximate methods, thus approximating problem becomes optimization problem.</p>



<p style="font-size:18px;">While dealing with the optimization task, KL divergence is used as a distance metric. But we do not minimize it. Instead, ELBO, which has close relation to KL divergence, provides an easier way to reach global optimum by iteratively increasing the lower bound. This step is done by coordinate descent or gradient descent to find the steepest points of factors.</p>



<p style="font-size:18px;">Another important theorem is mean-field variational inference. It allows approximating complicated structure of distributions and also is easier to track. Then each factor can be updated independently by steepest descent algorithms.</p>



<p style="font-size:18px;">These algorithms are difficult to find optimal points without assisting of the natural gradient since parameters are in Euclidean space while distance metric is in distribution space. Fisher information matrix, a transformation between two kinds of space, is a key contribution of natural gradient.</p>



<p style="font-size:18px;">Finally, Stochastic Variational Inference inherits ideas of Stochastic Gradient Descent and proposes terms of global and local parameters. They respectively hold full-batch gradients and mini-batch gradients of models. SVI achieves in improving the scalability of methods, making Variational Inference methods more wide-known today.</p>



<p></p>
</div></div>			</div><!-- .entry-content -->
